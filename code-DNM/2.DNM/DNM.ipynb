{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73e998f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T04:53:07.275671Z",
     "start_time": "2025-01-28T04:53:07.270645Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, mean_absolute_percentage_error, r2_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import joblib\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22b5f3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T04:53:09.382606Z",
     "start_time": "2025-01-28T04:53:09.373348Z"
    }
   },
   "outputs": [],
   "source": [
    "class Soma_old(nn.Module):\n",
    "    def __init__(self, kt, qs):\n",
    "        super(Soma_old, self).__init__()\n",
    "        self.kt = kt\n",
    "        self.qs = qs\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + torch.exp(-self.kt * (x - self.qs)))\n",
    "#         y = x\n",
    "        return y\n",
    "\n",
    "class Membrane(nn.Module):\n",
    "    def __init__(self,b):\n",
    "        super(Membrane, self).__init__()\n",
    "        self.params = nn.ParameterDict({'b': nn.Parameter(b)})\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mul(x,self.params['b'])\n",
    "        x = torch.sum(x, 1)\n",
    "        return x\n",
    "\n",
    "class Dendritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dendritic, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.prod(x, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Synapse_old(nn.Module):\n",
    "\n",
    "    def __init__(self, w, q, k):\n",
    "        super(Synapse_old, self).__init__()\n",
    "        self.params = nn.ParameterDict({'w': nn.Parameter(w)})\n",
    "        self.params.update({'q': nn.Parameter(q)})\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        num, _ = self.params['w'].shape\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = x.repeat((1, num, 1))\n",
    "        y = 1 / (1 + torch.exp(\n",
    "            torch.mul(-self.k, (torch.mul(x, self.params['w']) - self.params['q']))))\n",
    "        return y\n",
    "\n",
    "class DNM_Model(nn.Module):\n",
    "    def __init__(self, w, q, k, kt, b,qs):\n",
    "        super(DNM_Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Synapse_old(w, q, k),\n",
    "            Dendritic(),\n",
    "            Membrane(b),\n",
    "            Soma_old(k, qs)\n",
    "        )\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.q = nn.Parameter(q)\n",
    "        self.k = k\n",
    "        self.kt = kt\n",
    "        self.b = nn.Parameter(b)\n",
    "        self.qs = qs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b256e6db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T04:53:09.999070Z",
     "start_time": "2025-01-28T04:53:09.990558Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, batch_size, model, loss_func, optimizer, DEVICE):\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train.shape[0])\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        indices = perm[i:i + batch_size]\n",
    "        batch_X, batch_y = X_train[indices].to(DEVICE), y_train[indices].to(DEVICE)\n",
    "        y_pred = model(batch_X)\n",
    "        loss = loss_func(y_pred, batch_y)\n",
    "        sum_loss += loss.item()\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss = sum_loss / count\n",
    "    return average_loss\n",
    "\n",
    "def test(X_test, y_test, model, loss_func, DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        loss = loss_func(y_pred, y_test)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "975081b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T04:53:10.653951Z",
     "start_time": "2025-01-28T04:53:10.642744Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    z_scores = (df - df.mean()) / df.std()\n",
    "    outliers = (z_scores.abs() > 3).any(axis=1)\n",
    "    df = df[~outliers]\n",
    "    data = df.to_numpy()\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1]\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    x_max, _ = torch.max(X, dim=0)\n",
    "    x_min, _ = torch.min(X, dim=0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    y_max, _ = torch.max(Y, dim=0)\n",
    "    y_min, _ = torch.min(Y, dim=0)\n",
    "    Y = (Y - y_min) / (y_max - y_min)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=23)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9504e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T04:53:08.478867Z",
     "start_time": "2025-01-28T04:53:08.468271Z"
    }
   },
   "outputs": [],
   "source": [
    "def MAE_loss(y_pred, y_true):\n",
    "    error = torch.abs(y_pred - y_true)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def MSE_loss(y_pred, y_true):\n",
    "    error = torch.square(y_pred - y_true)\n",
    "    error = torch.mean(error)\n",
    "    return error\n",
    "\n",
    "\n",
    "def R_square_loss(y_pred, y_true):\n",
    "    y_mean = torch.mean(y_true)\n",
    "    ss_res = torch.sum(torch.pow(y_true - y_pred, 2))\n",
    "    ss_tot = torch.sum(torch.pow(y_true - y_mean, 2))\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edc1cf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:02:17.984921Z",
     "start_time": "2025-01-28T04:55:03.730882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/800], Loss on Test Data: 0.02590176\n",
      "Epoch [101/800], Loss on Test Data: 0.00661080\n",
      "Epoch [201/800], Loss on Test Data: 0.00657466\n",
      "Epoch [301/800], Loss on Test Data: 0.00657076\n",
      "Epoch [401/800], Loss on Test Data: 0.00657076\n",
      "Epoch [501/800], Loss on Test Data: 0.00657076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(R_square_loss(y_pred, y_test))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m test(X_test, y_test, model, criterion, DEVICE)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, batch_size, model, loss_func, optimizer, DEVICE)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size):\n\u001b[0;32m      7\u001b[0m     indices \u001b[38;5;241m=\u001b[39m perm[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m----> 8\u001b[0m     batch_X, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE), y_train[indices]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[0;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, batch_y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 800\n",
    "    batch_size = 64\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    M, D = 33, X_train.shape[1]\n",
    "    torch.cuda.manual_seed(20)\n",
    "    torch.cuda.manual_seed_all(20) \n",
    "    w = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    q = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    b = torch.randn(M, dtype=torch.float32, device=DEVICE)\n",
    "    k = 1\n",
    "    kt=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    qs=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    model = DNM_Model(w, q, k,kt, b, qs).to(DEVICE)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(X_train, y_train, batch_size, model, criterion, optimizer, DEVICE)\n",
    "        loss = test(X_test, y_test, model, criterion, DEVICE)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            joblib.dump(best_model, \"DNM_model.dat\") \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss on Test Data: {best_loss:.8f}')\n",
    "    \n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        y_pred = model(X_test)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    print(R_square_loss(y_pred, y_test))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "695da574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:04:56.035439Z",
     "start_time": "2025-01-28T05:04:56.000454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13054\\AppData\\Local\\Temp\\ipykernel_28348\\2337591828.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = torch.tensor(y_pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7582, device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "model = joblib.load(\"DNM_model.dat\")\n",
    "model.eval()  \n",
    "with torch.no_grad():  \n",
    "    y_pred = model(X_test)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "R_square_loss(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4dc524d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:16:57.378438Z",
     "start_time": "2025-01-28T05:16:57.303398Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1 / 10:\n",
      "R² Score: 0.710243\n",
      "\n",
      "Run 2 / 10:\n",
      "R² Score: 0.711555\n",
      "\n",
      "Run 3 / 10:\n",
      "R² Score: 0.708212\n",
      "\n",
      "Run 4 / 10:\n",
      "R² Score: 0.706669\n",
      "\n",
      "Run 5 / 10:\n",
      "R² Score: 0.712976\n",
      "\n",
      "Run 6 / 10:\n",
      "R² Score: 0.711760\n",
      "\n",
      "Run 7 / 10:\n",
      "R² Score: 0.707085\n",
      "\n",
      "Run 8 / 10:\n",
      "R² Score: 0.704024\n",
      "\n",
      "Run 9 / 10:\n",
      "R² Score: 0.714118\n",
      "\n",
      "Run 10 / 10:\n",
      "R² Score: 0.712069\n"
     ]
    }
   ],
   "source": [
    "# sensitivity analysis\n",
    "def sensitivity_analysis_with_perturbation(X_test, perturbation_factor=0.05):\n",
    "\n",
    "    # 在每个特征上进行扰动\n",
    "    perturbation = perturbation_factor * torch.randn_like(X_test)\n",
    "    X_test_perturbed = X_test + perturbation\n",
    "    return X_test_perturbed\n",
    "\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    # 加载已经训练好的模型\n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  # 设置为评估模式\n",
    "    \n",
    "    # 进行 10 次扰动分析和 R² 输出\n",
    "    for run in range(10):\n",
    "        print(f\"\\nRun {run + 1} / 10:\")\n",
    "\n",
    "        # 执行扰动分析\n",
    "        X_test_perturbed = sensitivity_analysis_with_perturbation(X_test, perturbation_factor=0.05)\n",
    "\n",
    "        # 使用模型进行预测\n",
    "        with torch.no_grad():  # 确保不会计算梯度\n",
    "            y_pred = model(X_test_perturbed)\n",
    "        \n",
    "        # 计算 R²\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_test_1 = y_test.cpu().numpy()\n",
    "        r2 = r2_score(y_test_1, y_pred)\n",
    "        \n",
    "        print(f\"R² Score: {r2:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c874f22e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:26:21.858519Z",
     "start_time": "2025-01-28T05:22:29.396802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss on Test Data: 0.05200727\n",
      "Epoch [101/300], Loss on Test Data: 0.05112347\n",
      "Epoch [201/300], Loss on Test Data: 0.02193140\n",
      "tensor(0.5182, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13054\\AppData\\Local\\Temp\\ipykernel_28348\\2118694101.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = torch.tensor(y_pred)\n"
     ]
    }
   ],
   "source": [
    "# Comparison of optimization algorithms\n",
    "# SGD\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 300\n",
    "    batch_size = 64\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    M, D = 33, X_train.shape[1]\n",
    "    torch.cuda.manual_seed(20)\n",
    "    torch.cuda.manual_seed_all(20) \n",
    "    w = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    q = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    b = torch.randn(M, dtype=torch.float32, device=DEVICE)\n",
    "    k = 1\n",
    "    kt=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    qs=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    model = DNM_Model(w, q, k,kt, b, qs).to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(X_train, y_train, batch_size, model, criterion, optimizer, DEVICE)\n",
    "        loss = test(X_test, y_test, model, criterion, DEVICE)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            joblib.dump(best_model, \"DNM_model.dat\") \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss on Test Data: {best_loss:.8f}')\n",
    "    \n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        y_pred = model(X_test)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    print(R_square_loss(y_pred, y_test))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "246d19f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:30:33.354183Z",
     "start_time": "2025-01-28T05:26:21.863436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss on Test Data: 0.01551471\n",
      "Epoch [101/300], Loss on Test Data: 0.00675118\n",
      "Epoch [201/300], Loss on Test Data: 0.00671349\n",
      "tensor(0.7538, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13054\\AppData\\Local\\Temp\\ipykernel_28348\\2756982503.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = torch.tensor(y_pred)\n"
     ]
    }
   ],
   "source": [
    "# Adagrad\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 300\n",
    "    batch_size = 64\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    M, D = 33, X_train.shape[1]\n",
    "    torch.cuda.manual_seed(20)\n",
    "    torch.cuda.manual_seed_all(20) \n",
    "    w = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    q = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    b = torch.randn(M, dtype=torch.float32, device=DEVICE)\n",
    "    k = 1\n",
    "    kt=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    qs=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    model = DNM_Model(w, q, k,kt, b, qs).to(DEVICE)\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=0.05)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(X_train, y_train, batch_size, model, criterion, optimizer, DEVICE)\n",
    "        loss = test(X_test, y_test, model, criterion, DEVICE)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            joblib.dump(best_model, \"DNM_model.dat\") \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss on Test Data: {best_loss:.8f}')\n",
    "    \n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        y_pred = model(X_test)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    print(R_square_loss(y_pred, y_test))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf6cbdb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:34:48.375217Z",
     "start_time": "2025-01-28T05:30:33.357394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss on Test Data: 0.01595180\n",
      "Epoch [101/300], Loss on Test Data: 0.00658613\n",
      "Epoch [201/300], Loss on Test Data: 0.00658613\n",
      "tensor(0.7586, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13054\\AppData\\Local\\Temp\\ipykernel_28348\\3161610075.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = torch.tensor(y_pred)\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 300\n",
    "    batch_size = 64\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    M, D = 33, X_train.shape[1]\n",
    "    torch.cuda.manual_seed(20)\n",
    "    torch.cuda.manual_seed_all(20) \n",
    "    w = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    q = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    b = torch.randn(M, dtype=torch.float32, device=DEVICE)\n",
    "    k = 1\n",
    "    kt=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    qs=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    model = DNM_Model(w, q, k,kt, b, qs).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(X_train, y_train, batch_size, model, criterion, optimizer, DEVICE)\n",
    "        loss = test(X_test, y_test, model, criterion, DEVICE)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            joblib.dump(best_model, \"DNM_model.dat\") \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss on Test Data: {best_loss:.8f}')\n",
    "    \n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        y_pred = model(X_test)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    print(R_square_loss(y_pred, y_test))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08a3b3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T05:39:02.246102Z",
     "start_time": "2025-01-28T05:34:48.377229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss on Test Data: 0.02518431\n",
      "Epoch [101/300], Loss on Test Data: 0.00661515\n",
      "Epoch [201/300], Loss on Test Data: 0.00658264\n",
      "tensor(0.7581, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13054\\AppData\\Local\\Temp\\ipykernel_28348\\1126140402.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = torch.tensor(y_pred)\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "def main():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    epochs = 300\n",
    "    batch_size = 64\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data('data_g.csv')\n",
    "\n",
    "    M, D = 33, X_train.shape[1]\n",
    "    torch.cuda.manual_seed(20)\n",
    "    torch.cuda.manual_seed_all(20) \n",
    "    w = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    q = torch.randn(M, D, dtype=torch.float32, device=DEVICE)\n",
    "    b = torch.randn(M, dtype=torch.float32, device=DEVICE)\n",
    "    k = 1\n",
    "    kt=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    qs=torch.randn(1, dtype=torch.float32, device=DEVICE)\n",
    "    model = DNM_Model(w, q, k,kt, b, qs).to(DEVICE)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(X_train, y_train, batch_size, model, criterion, optimizer, DEVICE)\n",
    "        loss = test(X_test, y_test, model, criterion, DEVICE)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            joblib.dump(best_model, \"DNM_model.dat\") \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss on Test Data: {best_loss:.8f}')\n",
    "    \n",
    "    model = joblib.load(\"DNM_model.dat\")\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        y_pred = model(X_test)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    print(R_square_loss(y_pred, y_test))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
